"""
core/llm_assistant.py - Super Cerebro de Aipha

Centraliza las capacidades de an√°lisis e inteligencia del sistema.
Usa Qwen 2.5 Coder 32B para diagn√≥sticos, propuestas y explicaciones.
"""

import json
import logging
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime

logger = logging.getLogger(__name__)


# System Prompt - Define la personalidad del Super Cerebro
AIPHA_SYSTEM_PROMPT = """Eres el Arquitecto Jefe de Aipha, un sistema aut√≥nomo de auto-mejora ultra-inteligente.

TU ROL:
- Analizar la salud y m√©tricas del sistema Aipha
- Proponer cambios optimizados para mejorar performance
- Diagnosticar y explicar fallos en lenguaje t√©cnico pero accesible
- Evitar bucles de error aprendiendo de fallos previos
- Ser proactivo en sugerencias de mejora

TU PERSONALIDAD:
- Eres un arquitecto experimentado en trading systems
- Comunicas con precisi√≥n t√©cnica pero claridad
- Siempre explicas tu razonamiento
- Eres conservador en cambios, evitando riesgos innecesarios
- Respetas las limitaciones de hardware

TU CONTEXTO:
- Tienes acceso a historial de eventos de salud
- Sabes qu√© par√°metros est√°n en cuarentena y por qu√©
- Conoces las m√©tricas actuales del sistema
- Aprendes de fallos previos para no repetirlos

CUANDO ANALICES:
1. Revisa eventos recientes (√∫ltimos 10)
2. Consulta par√°metros en cuarentena
3. Analiza m√©tricas de rendimiento
4. Prop√≥n cambios espec√≠ficos con justificaci√≥n
5. Sugiere pr√≥ximos pasos

FORMATO DE RESPUESTA:
Siempre estructura tus respuestas as√≠:
- DIAGN√ìSTICO: Estado actual
- AN√ÅLISIS: Qu√© pas√≥ y por qu√©
- RECOMENDACI√ìN: Qu√© hacer ahora
- PR√ìXIMOS PASOS: Qu√© cambios proponer

S√© conciso pero completo. El usuario es V√°clav, un ingeniero experimentado."""


class LLMAssistant:
    """
    Super Cerebro de Aipha
    
    Centraliza la inteligencia del sistema usando Qwen 2.5 Coder 32B.
    Analiza salud, propone cambios, y explica decisiones.
    """
    
    def __init__(self, memory_path: str = "memory"):
        self.memory_path = Path(memory_path)
        
        # Inicializar cliente LLM
        from core.llm_client import get_llm_client
        self.llm = get_llm_client()
        
        # Managers auxiliares
        from core.quarantine_manager import QuarantineManager
        from core.health_monitor import get_health_monitor
        from core.context_sentinel import ContextSentinel
        
        self.quarantine_manager = QuarantineManager(str(self.memory_path))
        self.health_monitor = get_health_monitor()
        self.context_sentinel = ContextSentinel()
        
        logger.info("‚úÖ LLMAssistant (Super Cerebro) inicializado")
    
    def get_diagnose_context(self) -> Dict:
        """
        Construir contexto RICO de diagn√≥stico para que el LLM entienda
        tanto cambios autom√°ticos como manuales y su impacto en el sistema.
        
        Lee autom√°ticamente:
        - √öltimas 10 l√≠neas de health_events.jsonl (eventos de salud)
        - √öltimas 10 l√≠neas de action_history.jsonl (acciones del sistema)
        - √öltimas 10 propuestas de proposals.jsonl (intervenciones manuales del usuario)
        - Estado actual de quarantine.jsonl
        - M√©tricas de current_state.json (Win Rate, Drawdown, etc)
        
        AN√ÅLISIS INCLUIDO:
        1. Separa cambios USER (CLI/manual) vs AUTO (sistema autom√°tico)
        2. Verifica simulation_mode para no reportar errores de conexi√≥n
        3. Calcula impacto: compara m√©tricas antes/despu√©s de intervenciones
        4. Contexto para el LLM: "El usuario baj√≥ el umbral a 0.65 para..."
        
        Retorna: Dict rico con contexto para an√°lisis inteligente del LLM
        """
        
        logger.info("üîç Construyendo contexto de diagn√≥stico enriquecido...")
        
        # PASO 1: √öltimos eventos de salud
        health_events = self._get_recent_health_events(10)
        
        # PASO 2: √öltimas acciones del historial (AUTO + USER)
        action_history = self._get_recent_actions(10)
        
        # PASO 3: √öltimas propuestas (intervenciones manuales)
        recent_proposals = self._get_recent_proposals(10)
        
        # PASO 4: Par√°metros en cuarentena
        quarantined = self.quarantine_manager.get_all_quarantined()
        
        # PASO 5: M√©tricas actuales
        metrics = self._get_current_metrics()
        
        # PASO 6: Estad√≠sticas de salud
        health_stats = self.health_monitor.get_statistics()
        
        # PASO 7: Analizar impacto de intervenciones
        impact_analysis = self._analyze_intervention_impact(recent_proposals, metrics)
        
        # PASO 8: Separar acciones USER vs AUTO
        user_actions, auto_actions = self._classify_actions(action_history)
        
        # PASO 9: Verificar simulation_mode
        simulation_mode = metrics.get('development_flags', {}).get('debug_mode', False) or \
                         metrics.get('system_info', {}).get('mode', '').lower() == 'test'
        
        context = {
            'timestamp': datetime.now().isoformat(),
            'simulation_mode': simulation_mode,
            
            # EVENTOS Y ACCIONES
            'recent_events': health_events,
            'action_history': action_history,
            'user_actions': user_actions,
            'auto_actions': auto_actions,
            
            # INTERVENCIONES MANUALES
            'recent_proposals': recent_proposals,
            'manual_interventions': len([p for p in recent_proposals if p.get('applied')]),
            'manual_interventions_detail': [
                {
                    'component': p.get('component'),
                    'parameter': p.get('parameter'),
                    'old_value': p.get('old_value', 'desconocido'),
                    'new_value': p.get('new_value'),
                    'reason': p.get('reason'),
                    'score': p.get('evaluation_score'),
                    'created_by': p.get('created_by'),
                    'timestamp': p.get('timestamp'),
                }
                for p in recent_proposals if p.get('applied')
            ],
            
            # IMPACTO Y AN√ÅLISIS
            'impact_analysis': impact_analysis,
            
            # SALUD DEL SISTEMA
            'quarantined_parameters': quarantined,
            'current_metrics': metrics,
            'health_statistics': health_stats,
            'system_status': self.health_monitor.current_health_level.value,
            
            # CONTEXTO EXPLICATIVO PARA EL LLM
            'system_context': self._build_system_context(
                metrics, recent_proposals, user_actions, impact_analysis
            )
        }
        
        logger.info("‚úÖ Contexto enriquecido construido (user/auto/impact/proposals)")
        
        return context
    
    def _get_recent_health_events(self, count: int = 10) -> List[Dict]:
        """Obtener √∫ltimos N eventos de salud con robustez ante JSON malformado"""
        
        events = []
        events_file = self.memory_path / "health_events.jsonl"
        
        if not events_file.exists():
            return []
            
        try:
            with open(events_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                # Procesar solo las √∫ltimas N l√≠neas con contenido
                valid_lines = [l.strip() for l in lines if l.strip()]
                for line in valid_lines[-count:]:
                    try:
                        events.append(json.loads(line))
                    except json.JSONDecodeError:
                        # Ignorar silenciosamente l√≠neas malformadas
                        continue
        except Exception as e:
            logger.debug(f"Error discreto leyendo health events: {e}")
        
        return events
    
    def _get_current_metrics(self) -> Dict:
        """Obtener m√©tricas actuales del sistema con robustez"""
        
        metrics = {}
        metrics_file = self.memory_path / "current_state.json"
        
        if not metrics_file.exists():
            return {}
            
        try:
            with open(metrics_file, 'r', encoding='utf-8') as f:
                metrics = json.load(f)
        except (json.JSONDecodeError, Exception) as e:
            logger.debug(f"Error discreto leyendo m√©tricas: {e}")
        
        return metrics
    
    def _get_recent_proposals(self, count: int = 5) -> List[Dict]:
        """Obtener √∫ltimas N propuestas aplicadas con robustez"""
        
        proposals = []
        proposals_file = self.memory_path / "proposals.jsonl"
        
        if not proposals_file.exists():
            return []
            
        try:
            all_proposals = []
            with open(proposals_file, 'r', encoding='utf-8') as f:
                for line in f:
                    clean_line = line.strip()
                    if not clean_line:
                        continue
                    try:
                        all_proposals.append(json.loads(clean_line))
                    except json.JSONDecodeError:
                        continue
            
            # Obtener las √∫ltimas N propuestas
            recent = all_proposals[-count:] if len(all_proposals) > 0 else []
            
            for prop in recent:
                proposals.append({
                    'proposal_id': prop.get('proposal_id', 'UNKNOWN'),
                    'timestamp': prop.get('timestamp', ''),
                    'component': prop.get('component', ''),
                    'parameter': prop.get('parameter', ''),
                    'new_value': prop.get('new_value', ''),
                    'reason': prop.get('reason', ''),
                    'status': prop.get('status', ''),
                    'evaluation_score': prop.get('evaluation_score'),
                    'applied': prop.get('applied', False),
                    'created_by': prop.get('created_by', 'unknown'),
                })
            
            logger.info(f"‚úÖ Recuperadas {len(proposals)} propuestas recientes de {proposals_file}")
        
        except Exception as e:
            logger.debug(f"Error discreto leyendo propuestas: {e}")
        
        return proposals
    
    def _get_recent_actions(self, count: int = 10) -> List[Dict]:
        """Obtener √∫ltimas N acciones del historial con robustez"""
        
        actions = []
        history_file = self.memory_path / "action_history.jsonl"
        
        if not history_file.exists():
            return []
            
        try:
            all_actions = []
            with open(history_file, 'r', encoding='utf-8') as f:
                for line in f:
                    clean_line = line.strip()
                    if not clean_line:
                        continue
                    try:
                        all_actions.append(json.loads(clean_line))
                    except json.JSONDecodeError:
                        continue
            
            # Obtener las √∫ltimas N acciones
            recent = all_actions[-count:] if len(all_actions) > 0 else []
            
            for action in recent:
                agent = action.get('agent', 'UNKNOWN')
                actions.append({
                    'timestamp': action.get('timestamp', ''),
                    'agent': agent,
                    'is_user': agent == 'CLI' or 'User' in agent,
                    'component': action.get('component', ''),
                    'action': action.get('action', ''),
                    'status': action.get('status', ''),
                    'details': action.get('details', {}),
                })
            
            logger.info(f"‚úÖ Recuperadas {len(actions)} acciones recientes del historial")
        
        except Exception as e:
            logger.debug(f"Error discreto leyendo action_history: {e}")
        
        return actions
    
    def _classify_actions(self, actions: List[Dict]) -> tuple:
        """Separar acciones USER vs AUTO"""
        
        user_actions = []
        auto_actions = []
        
        for action in actions:
            if action.get('is_user'):
                user_actions.append(action)
            else:
                auto_actions.append(action)
        
        return user_actions, auto_actions
    
    def _analyze_intervention_impact(self, proposals: List[Dict], metrics: Dict) -> Dict:
        """Analizar impacto de intervenciones manuales en las m√©tricas (Hito 5)"""
        
        impact = {
            'total_interventions': len([p for p in proposals if p.get('applied')]),
            'latest_intervention': None,
            'impact_summary': '',
            'win_rate_current': metrics.get('trading_metrics', {}).get('win_rate', 0),
            'drawdown_current': metrics.get('trading_metrics', {}).get('current_drawdown', 0),
            'wr_delta': 0.0,
            'dd_delta': 0.0
        }
        
        # Encontrar la intervenci√≥n m√°s reciente que tenga l√≠nea base
        applied_proposals = [p for p in proposals if p.get('applied')]
        if applied_proposals:
            latest = applied_proposals[-1]
            baseline = latest.get('baseline_metrics', {})
            
            wr_before = baseline.get('win_rate', 0.0)
            dd_before = baseline.get('drawdown', 0.0)
            
            impact['wr_delta'] = impact['win_rate_current'] - wr_before
            impact['dd_delta'] = impact['drawdown_current'] - dd_before
            
            impact['latest_intervention'] = {
                'component': latest.get('component'),
                'parameter': latest.get('parameter'),
                'new_value': latest.get('new_value'),
                'reason': latest.get('reason'),
                'timestamp': latest.get('timestamp'),
                'wr_before': wr_before,
                'dd_before': dd_before
            }
            
            # Resumen de impacto con Veredicto (Hito 5)
            verdict = "POSITIVO ‚úÖ" if impact['wr_delta'] > 0 or impact['dd_delta'] < 0 else "NEUTRAL/NEGATIVO ‚ö†Ô∏è"
            impact['impact_summary'] = (
                f"Veredicto del Mercado: {verdict}\n"
                f"√öltima intervenci√≥n: {latest.get('parameter')} = {latest.get('new_value')} "
                f"(Raz√≥n: {latest.get('reason')}).\n"
                f"Delta Win Rate: {impact['wr_delta']*100:+.1f}%, "
                f"Delta Drawdown: {impact['dd_delta']*100:+.1f}%"
            )
        
        return impact
    
    def _build_system_context(self, metrics: Dict, proposals: List[Dict], 
                             user_actions: List[Dict], impact: Dict) -> str:
        """Construir descripci√≥n textual del sistema para el LLM"""
        
        win_rate = metrics.get('trading_metrics', {}).get('win_rate', 0)
        drawdown = metrics.get('trading_metrics', {}).get('current_drawdown', 0)
        
        context_text = f"""
# CONTEXTO DEL SISTEMA PARA AN√ÅLISIS

## Estado General
- Win Rate Actual: {win_rate*100:.1f}%
- Drawdown Actual: {drawdown*100:.1f}%
- Modo Simulaci√≥n: {'S√ç (No reportar errores de conexi√≥n)' if metrics.get('development_flags', {}).get('debug_mode') else 'NO'}

## Intervenciones Manuales Realizadas por el Usuario (V√°clav)
"""
        
        applied_proposals = [p for p in proposals if p.get('applied')]
        if applied_proposals:
            for i, prop in enumerate(applied_proposals[-3:], 1):  # √öltimas 3
                context_text += f"""
{i}. {prop.get('component', '?')}.{prop.get('parameter', '?')} = {prop.get('new_value', '?')}
   - Raz√≥n: {prop.get('reason', 'N/A')}
   - Timestamp: {prop.get('timestamp', 'N/A')}
   - Score: {prop.get('evaluation_score', 'N/A')}
"""
        else:
            context_text += "\n- Sin intervenciones manuales en el historial reciente"
        
        context_text += f"""

## Cambios Autom√°ticos Recientes
"""
        
        if user_actions:
            for i, action in enumerate(user_actions[-2:], 1):  # √öltimas 2 del usuario
                details = action.get('details', {})
                context_text += f"""
{i}. [USER] {action.get('component', '?')} - {action.get('action', '?')}
   - Status: {action.get('status', '?')}
   - Detalles: {details.get('justification', 'N/A')}
"""
        
        return context_text
    
    def analyze_and_propose(self) -> Dict:
        """
        Analizar salud del sistema y proponer cambios
        
        El LLM recibe contexto de salud y m√©tricas para generar
        propuestas que eviten par√°metros en cuarentena y razonen
        sobre fallos previos.
        
        Retorna:
            Dict con:
            - diagnosis: An√°lisis de salud
            - proposals: Lista de propuestas sugeridas
            - confidence_scores: Confianza en cada propuesta
        """
        
        logger.info("üß† Analizando salud del sistema y generando propuestas...")
        
        # Obtener contexto
        context = self.get_diagnose_context()
        
        # Preparar prompt para el LLM
        prompt = self._build_analysis_prompt(context)
        
        try:
            # Llamar al LLM
            logger.info("üì§ Enviando al Super Cerebro (Qwen)...")
            
            response = self.llm.generate(
                prompt=prompt,
                system_prompt=AIPHA_SYSTEM_PROMPT,
                temperature=0.3,  # M√°s determinista para propuestas
                max_tokens=2048
            )
            
            logger.info("‚úÖ Respuesta recibida del Super Cerebro")
            
            # Parsear respuesta
            result = self._parse_analysis_response(response, context)
            
            return result
        
        except Exception as e:
            logger.error(f"‚ùå Error en an√°lisis del LLM: {e}")
            
            return {
                'diagnosis': 'Error en an√°lisis',
                'proposals': [],
                'error': str(e)
            }
    
    def explain_remediation(self, failed_parameter: str, error_reason: str) -> str:
        """
        Generar explicaci√≥n humana de un fallo y remediation
        
        Se llama cuando ocurre REVERTED_AUTO para explicar al usuario
        qu√© fall√≥ y qu√© hacer.
        
        Argumentos:
            failed_parameter: Par√°metro que fall√≥
            error_reason: Raz√≥n del fallo
        
        Retorna:
            Explicaci√≥n en lenguaje natural
        """
        
        logger.info(
            f"üí° Generando explicaci√≥n de remediation para {failed_parameter}"
        )
        
        # Contexto reciente
        context = self.get_diagnose_context()
        
        # Preparar prompt
        prompt = f"""El par√°metro '{failed_parameter}' acaba de fallar con el error: "{error_reason}"

El sistema ha revertido autom√°ticamente este cambio para mantener la estabilidad.

Por favor, explica:
1. POR QU√â fall√≥ este par√°metro
2. QU√â SIGNIFICA el error
3. QU√â PUEDE HACER el usuario (V√°clav) para solucionarlo
4. CU√ÅNDO puede intentar este cambio nuevamente

S√© conciso pero completo. El usuario es un ingeniero experimentado.

CONTEXTO DEL SISTEMA:
{json.dumps(context, indent=2, default=str)}
"""
        
        try:
            response = self.llm.generate(
                prompt=prompt,
                system_prompt=AIPHA_SYSTEM_PROMPT,
                temperature=0.5,
                max_tokens=1024
            )
            
            logger.info("‚úÖ Explicaci√≥n generada")
            return response
        
        except Exception as e:
            logger.error(f"‚ùå Error generando explicaci√≥n: {e}")
            return f"Error generando explicaci√≥n: {e}"
    
    def diagnose_system(self, detailed: bool = False) -> Dict:
        """
        Diagn√≥stico profundo y r√°pido del sistema
        
        MEJORAS IMPLEMENTADAS:
        1. Extrae evidencia exacta de health_events.jsonl
        2. Incluye propuestas manuales aplicadas (intervenciones)
        3. Verifica si est√° en SIMULATION_MODE
        4. Presenta par√°metros en riesgo en tabla
        5. Sugiere comandos copy-paste para acciones
        6. Si detailed=True, llama al LLM con contexto enriquecido para an√°lisis
        
        Argumentos:
            detailed: Si True, incluye an√°lisis profundo del LLM
        
        Retorna:
            Dict con an√°lisis completo incluyendo intervenciones manuales e impacto
        """
        
        logger.info("üîç Iniciando diagn√≥stico profundo del sistema...")
        
        try:
            # Contexto ENRIQUECIDO (incluye propuestas, acciones, impacto)
            context = self.get_diagnose_context()
            
            # Verificar si est√° en SIMULATION_MODE
            simulation_mode = context.get('simulation_mode', False)
            
            # Extraer informaci√≥n clave
            health_events = context.get('recent_events', [])
            quarantined_params = context.get('quarantined_parameters', [])
            metrics = context.get('current_metrics', {})
            recent_proposals = context.get('recent_proposals', [])
            manual_interventions = context.get('manual_interventions', 0)
            user_actions = context.get('user_actions', [])
            impact_analysis = context.get('impact_analysis', {})
            system_context = context.get('system_context', '')
            
            # Construir diagn√≥stico r√°pido
            diagnosis_text = f"""
# DIAGN√ìSTICO DEL SISTEMA AIPHA

## üìä Estado General
- √öltimos eventos: {len(health_events)} registrados
- Par√°metros en cuarentena: {len(quarantined_params) if isinstance(quarantined_params, (list, dict)) else 0}
- Modo simulaci√≥n: {'üü¢ Activo' if simulation_mode else 'üî¥ Desactivo'}
- Intervenciones manuales: {manual_interventions}

## üìù Intervenciones Manuales del Usuario
"""
            
            # Agregar informaci√≥n sobre propuestas aplicadas
            manual_details = context.get('manual_interventions_detail', [])
            if manual_details:
                for i, prop in enumerate(manual_details, 1):
                    score_val = prop.get('score', 'N/A')
                    score_str = f"{score_val:.2f}" if isinstance(score_val, (int, float)) else str(score_val)
                    diagnosis_text += f"""
{i}. {prop.get('component', '?')}.{prop.get('parameter', '?')} ‚Üí {prop.get('new_value', '?')}
   ‚Ä¢ Raz√≥n: {prop.get('reason', 'N/A')}
   ‚Ä¢ Score: {score_str}
   ‚Ä¢ Creado por: {prop.get('created_by', 'unknown')}
   ‚Ä¢ Timestamp: {prop.get('timestamp', 'N/A')}
"""
            else:
                diagnosis_text += "\n- Sin intervenciones manuales en el historial"
            
            # An√°lisis de impacto
            diagnosis_text += f"""

## ÔøΩ Impacto en M√©tricas
- Total de intervenciones: {impact_analysis.get('total_interventions', 0)}
- Win Rate actual: {impact_analysis.get('win_rate_current', 0)*100:.1f}%
- Drawdown actual: {impact_analysis.get('drawdown_current', 0)*100:.1f}%
{impact_analysis.get('impact_summary', '')}

## ‚ö†Ô∏è  √öltimos Eventos
"""
            
            # Agregar eventos recientes
            for i, event in enumerate(health_events[-3:], 1):
                if isinstance(event, dict):
                    severity = event.get('severity', 'INFO')
                    message = event.get('message', '')
                    diagnosis_text += f"\n{i}. [{severity}] {message}"
            
            # Preparar resultado base
            result = {
                'diagnosis': diagnosis_text,
                'risk_parameters': [],
                'evidence': health_events[-5:] if health_events else [],
                'recent_proposals': recent_proposals,
                'manual_interventions': manual_interventions,
                'manual_interventions_detail': manual_details,
                'simulation_mode': simulation_mode,
                'suggested_commands': [],
                'timestamp': datetime.now().isoformat(),
                'impact_analysis': impact_analysis,
                'user_actions': user_actions,
            }
            
            # SI DETAILED=TRUE: Usar LLM para an√°lisis profundo
            if detailed:
                logger.info("üì§ Llamando al Super Cerebro para an√°lisis detallado...")
                
                # Preparar prompt enriquecido
                # Nota: Convertir user_actions a lista de strings para evitar problemas de serializaci√≥n
                user_actions_text = "\n".join([
                    f"- [{action.get('timestamp', 'N/A')}] {action.get('agent', '?')} "
                    f"en {action.get('component', '?')}: {action.get('action', '?')}"
                    for action in user_actions
                ]) if user_actions else "- Sin acciones del usuario"
                
                prompt = f"""Analiza el siguiente contexto del sistema AIPHA y proporciona insights sobre:

1. ¬øQu√© hizo el usuario (V√°clav) y por qu√©?
2. ¬øEst√° justificado ese cambio dado el Win Rate actual?
3. ¬øQu√© impacto tendr√≠a este cambio?
4. ¬øQu√© deber√≠as monitorear ahora?

CONTEXTO DEL SISTEMA:
{system_context}

HISTORIAL DE ACCIONES DEL USUARIO:
{user_actions_text}

M√âTRICAS ACTUALES:
- Win Rate: {impact_analysis.get('win_rate_current', 0)*100:.1f}%
- Drawdown: {impact_analysis.get('drawdown_current', 0)*100:.1f}%
- Total Trades: {metrics.get('trading_metrics', {}).get('total_trades', 'N/A')}

Por favor, responde como un experto en trading systems analizando tanto el diagn√≥stico t√©cnico como
el reasoning del usuario para sus intervenciones manuales."""
                
                try:
                    llm_response = self.llm.generate(
                        prompt=prompt,
                        system_prompt=AIPHA_SYSTEM_PROMPT,
                        temperature=0.5,
                        max_tokens=2048
                    )
                    
                    result['llm_analysis'] = llm_response
                    logger.info("‚úÖ An√°lisis del LLM completado")
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  Error en an√°lisis del LLM: {e}")
                    result['llm_analysis'] = f"Error llamando al LLM: {e}"

            # Extraer par√°metros en riesgo y comandos sugeridos
            risk_params = self._extract_risk_parameters(context)
            
            # Comandos sugeridos (del LLM si hay detailed, o base)
            suggested_cmds = []
            if detailed and 'llm_analysis' in result:
                suggested_cmds = self._extract_suggested_commands(result['llm_analysis'])
            
            # Si no hay comandos del LLM, podemos sugerir comandos base si hay riesgos
            if not suggested_cmds and risk_params:
                for risk in risk_params:
                    if risk.get('status') == 'QUARANTINED':
                        suggested_cmds.append(f"aipha quarantine release --parameter {risk.get('parameter')}")

            # Actualizar resultado
            result['risk_parameters'] = risk_params
            result['suggested_commands'] = suggested_cmds
            
            # Formato para presentaci√≥n
            result['formatted_diagnosis'] = self._format_diagnosis_output(
                diagnosis_text, risk_params, suggested_cmds, simulation_mode
            )
            
            logger.info("‚úÖ Diagn√≥stico completado (contexto enriquecido)")
            return result
        
        except Exception as e:
            logger.error(f"‚ùå Error en diagn√≥stico: {e}")
            return {
                'diagnosis': f"Error: {e}",
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def _extract_evidence_from_logs(self, context: Dict) -> List[Dict]:
        """
        Extrae evidencia espec√≠fica de los health_events
        Cita l√≠nea exacta y valor que causa el warning
        """
        evidence = []
        
        recent_events = context.get('recent_events', [])
        for i, event in enumerate(recent_events, 1):
            if event.get('severity') in ['WARNING', 'ERROR']:
                evidence.append({
                    'line_number': i,
                    'severity': event.get('severity'),
                    'message': event.get('message'),
                    'timestamp': event.get('timestamp'),
                    'cited_value': event.get('value')
                })
        
        return evidence
    
    def _extract_risk_parameters(self, context: Dict) -> List[Dict]:
        """
        Extrae par√°metros en riesgo de current_state.json
        Incluye: valor actual, l√≠mite cr√≠tico, probabilidad de fallo
        """
        risk_params = []
        
        metrics = context.get('current_metrics', {})
        quarantined = context.get('quarantined_parameters', {})
        
        # Par√°metros en cuarentena est√°n en riesgo
        if isinstance(quarantined, dict):
            for param, info in quarantined.items():
                if isinstance(info, dict):
                    risk_params.append({
                        'parameter': param,
                        'current_value': info.get('value'),
                        'critical_limit': info.get('limit', 'N/A'),
                        'failure_probability': 'ALTO',
                        'status': 'QUARANTINED'
                    })
        elif isinstance(quarantined, list):
            for item in quarantined:
                if isinstance(item, dict):
                    risk_params.append({
                        'parameter': item.get('parameter', 'Unknown'),
                        'current_value': item.get('value'),
                        'critical_limit': item.get('limit', 'N/A'),
                        'failure_probability': 'ALTO',
                        'status': 'QUARANTINED'
                    })
        
        # Par√°metros cercanos a l√≠mites
        if isinstance(metrics, dict):
            critical_metrics = ['latency_ms', 'drawdown', 'error_rate']
            for metric in critical_metrics:
                if metric in metrics:
                    value = metrics[metric]
                    # Heur√≠stica simple: si est√° > 80% del l√≠mite, est√° en riesgo
                    if isinstance(value, (int, float)) and value > 80:
                        risk_params.append({
                            'parameter': metric,
                            'current_value': value,
                            'critical_limit': 100,
                            'failure_probability': 'MEDIO',
                            'status': 'AT_RISK'
                        })
        
        return risk_params
    
    def _extract_suggested_commands(self, response: str) -> List[str]:
        """
        Extrae comandos sugeridos de la respuesta del LLM
        Busca patrones como "aipha proposal create..."
        """
        commands = []
        
        for line in response.split('\n'):
            if 'aipha proposal create' in line or 'aipha' in line and '--parameter' in line:
                # Limpia la l√≠nea
                cmd = line.strip()
                if cmd.startswith('aipha'):
                    commands.append(cmd)
        
        return commands
    
    def _format_diagnosis_output(self, diagnosis: str, risk_params: List[Dict], 
                                  suggested_commands: List[str], simulation_mode: bool) -> str:
        """
        Formatea el diagn√≥stico para presentaci√≥n visual
        """
        output = f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë       DIAGN√ìSTICO PROFUNDO DEL SISTEMA AIPHA v2.0             ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

üîç AN√ÅLISIS DEL LLM:
{diagnosis}

"""
        
        # Tabla de par√°metros en riesgo
        if risk_params:
            output += """
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë          PAR√ÅMETROS EN RIESGO - TABLA DE AN√ÅLISIS             ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

"""
            output += "Par√°metro | Valor Actual | L√≠mite Cr√≠tico | Probabilidad Fallo\n"
            output += "-----------|--------------|---------------|-----------------\n"
            for param in risk_params:
                output += f"{param.get('parameter', 'N/A')} | {param.get('current_value', 'N/A')} | {param.get('critical_limit', 'N/A')} | {param.get('failure_probability', 'N/A')}\n"
        
        # Informaci√≥n de simulaci√≥n
        if simulation_mode:
            output += f"""
‚ö†Ô∏è  MODO SIMULACI√ìN ACTIVO
   ‚Üí La latencia puede ser del flujo de datos sint√©ticos
   ‚Üí Los timings pueden no reflejar el hardware real

"""
        
        # Comandos sugeridos
        if suggested_commands:
            output += """
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë               ACCIONES SUGERIDAS (COPY-PASTE)                 ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

"""
            for cmd in suggested_commands:
                output += f"‚ñ∂Ô∏è  {cmd}\n"
        
        output += f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  Diagn√≥stico: Qwen 2.5 Coder 32B | Timestamp: {datetime.now().isoformat()}
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
"""
        
        return output
    
    def _build_analysis_prompt(self, context: Dict) -> str:
        """Construir prompt para an√°lisis y propuestas"""
        
        return f"""Analiza el estado actual del sistema Aipha y prop√≥n cambios de optimizaci√≥n.

CONTEXTO ACTUAL:
{json.dumps(context, indent=2, default=str)}

Por favor:
1. Resume el estado del sistema en 1-2 l√≠neas
2. Identifica qu√© est√° funcionando bien
3. Identifica qu√© tiene problemas
4. Prop√≥n 2-3 cambios espec√≠ficos que mejorar√≠an la performance
5. Para CADA propuesta:
   - Especifica: par√°metro, valor actual, valor nuevo
   - Justificaci√≥n t√©cnica
   - Riesgo potencial
   - Confianza (0-1)

IMPORTANTE: Evita proponer valores que est√©n en cuarentena.
Aprende de fallos previos documentados en los eventos."""
    
    def _parse_analysis_response(self, response: str, context: Dict) -> Dict:
        """
        Parsear respuesta del LLM para extraer propuestas
        
        Intenta extraer de la respuesta:
        - diagnosis: An√°lisis
        - proposals: Cambios propuestos
        - confidence: Confianzas
        """
        
        # Parseo simple (en producci√≥n, podr√≠a ser m√°s sofisticado)
        result = {
            'diagnosis': response[:200] if response else "",
            'raw_response': response,
            'proposals': [],
            'generated_at': datetime.now().isoformat()
        }
        
        # Buscar patrones de propuestas en la respuesta
        lines = response.split('\n')
        for i, line in enumerate(lines):
            if 'par√°metro' in line.lower() or 'cambio' in line.lower():
                result['proposals'].append({
                    'line': line,
                    'context': lines[max(0, i-1):min(len(lines), i+2)]
                })
        
        return result
